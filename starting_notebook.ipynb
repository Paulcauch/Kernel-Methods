{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy.optimize import fmin_l_bfgs_b \n",
    "from cvxopt import matrix, solvers\n",
    "import pickle as pkl\n",
    "from scipy import optimize\n",
    "from scipy.linalg import cho_factor, cho_solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr = np.array(pd.read_csv('data_image/Xtr.csv',header=None,sep=',',usecols=range(3072))) \n",
    "Xte = np.array(pd.read_csv('data_image/Xte.csv',header=None,sep=',',usecols=range(3072))) \n",
    "Ytr = np.array(pd.read_csv('data_image/Ytr.csv',sep=',',usecols=[1])).squeeze() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIZUALISATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images_grid(data, nrows, ncols):\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols*2, nrows*2))\n",
    "    random=np.random.choice(data.shape[0],size=nrows*ncols)\n",
    "    #data=(data-np.min(data))/(np.max(data)-np.min(data))\n",
    "    for j, ax in enumerate(axes.flat):\n",
    "        i=random[j]\n",
    "        if i < data.shape[0]:\n",
    "            image_data = data[i, :]\n",
    "            \n",
    "            # Normaliser les donnÃ©es dans l'intervalle [0, 1]\n",
    "            # min_val = image_data.min()\n",
    "            # max_val = image_data.max()\n",
    "            # print(min_val,max_val)\n",
    "            # image_data = (image_data - min_val) / (max_val - min_val)\n",
    "            # print(image_data.max())\n",
    "            \n",
    "            red_channel = image_data[:1024].reshape((32, 32))\n",
    "            red_channel=(red_channel-red_channel.min())/(red_channel.max()-red_channel.min())\n",
    "            green_channel = image_data[1024:2048].reshape((32, 32))\n",
    "            green_channel=(green_channel-green_channel.min())/(green_channel.max()-green_channel.min())\n",
    "            blue_channel = image_data[2048:].reshape((32, 32))\n",
    "            blue_channel=(blue_channel-blue_channel.min())/(blue_channel.max()-blue_channel.min())\n",
    "            image = np.stack((red_channel, green_channel, blue_channel), axis=-1)\n",
    "\n",
    "            ax.imshow(image)\n",
    "            ax.axis('off')\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "plot_images_grid(Xtr,2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KERNELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some kernels :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBF():\n",
    "    \"\"\"\n",
    "    Compute the matrix of the Gaussian Kernel : \n",
    "    \n",
    "    (K)_ij=K(X_i,Y_j)=exp( 1/2*sigma^2 * ||X_i-Y_j||^2 ) the Gaussian Kernel evaluated between the ith data and jth data\n",
    "\n",
    "    Parameters : \n",
    "    X : 2d array size (n,p) \n",
    "        the Data matrix with n the number of data and p the size of data\n",
    "    Y : 2d array size (q,p) \n",
    "        the Data matrix with q the number of data and p the size of data\n",
    "    sigma : float \n",
    "             Variance of the GaussianKernel \n",
    "\n",
    "    Outputs :\n",
    "    2d array size (n,q)\n",
    "    Matrix of the Gaussian Kernel\n",
    "    \"\"\"   \n",
    "    def __init__(self, sigma=1.):\n",
    "        self.sigma = sigma  ## the variance of the kernel\n",
    "    def kernel(self,X,Y):\n",
    "        ## Input vectors X and Y of shape Nxd and Mxd\n",
    "        diff2 = np.sum(X**2, axis=1)[:, None] + np.sum(Y**2, axis=1)[None, :] - 2 * np.dot(X, Y.T)\n",
    "        return  np.exp(-diff2/(2*self.sigma**2)) ## Matrix of shape NxM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class polynomial():\n",
    "    \"\"\"\n",
    "    Compute the matrix of the Polynomial Kernel : \n",
    "    \n",
    "    (K)_ij=K(X_i,Y_j)=(<X_i,Y_j>)^d the Polynomial Kernel of degree d evaluated between the ith data and jth data\n",
    "\n",
    "    Parameters : \n",
    "    X : 2d array size (n,p) \n",
    "        the Data matrix with n the number of data and p the size of data\n",
    "    Y : 2d array size (q,p) \n",
    "        the Data matrix with q the number of data and p the size of data\n",
    "    d : Integer\n",
    "            Degree of the polynomial kernel\n",
    "\n",
    "    Outputs :\n",
    "    2d array size (n,q)\n",
    "    Matrix of the Gaussian Kernel\n",
    "    \"\"\"\n",
    "    def __init__(self,d=2):\n",
    "        self.d=d\n",
    "\n",
    "    def kernel(self,X,Y):\n",
    "        return np.dot(X,Y.T) ** self.d\n",
    "        # X_intercept = np.concatenate((X,np.ones(X.shape[0]).reshape(-1,1)), axis=1)\n",
    "        # Y_intercept= np.concatenate((Y,np.ones(Y.shape[0]).reshape(-1,1)), axis=1)\n",
    "        # return np.sum(X_intercept * Y_intercept[:,None,:], axis=-1) ** self.d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kernel_ridge_reg():\n",
    "    \"\"\"\n",
    "    Class which comput the solution to the Kernel Ridge Regression with regularization parameter lambda (lmbda)\n",
    "\n",
    "    ----> We have a close form for the solution : \n",
    "            alpha=(K+lambda*n*I)^-1y\n",
    "            pred=K@alpha\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,lmbda):\n",
    "        self.lmbda=lmbda\n",
    "\n",
    "    def train(self,K,y):\n",
    "        mat=K+self.lmbda*K.shape[0]*np.identity(K.shape[0])\n",
    "        self.alpha=scipy.linalg.solve(mat,y)\n",
    "    \n",
    "    def fit(self,K):\n",
    "        print(self.alpha.shape,K.shape)\n",
    "        return self.alpha@K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kernel_logistic_reg():\n",
    "    \"\"\"\n",
    "    Class which comput the solution to the Kernel logistic Regression with regularization parameter lambda (lmbda)\n",
    "\n",
    "    ----> We have to solve the smooth convex opti problem : \n",
    "            alpha=argmin 1/n sum_i=1^n logisitic(y_i[Kalpha]_i)+lambda/2 *alpha^T@K@alpha\n",
    "\n",
    "          We solve this with the L-FBGS form scipy\n",
    "\n",
    "          and then : \n",
    "            pred=K@alpha\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,lmbda,alpha0):\n",
    "        self.lmbda=lmbda\n",
    "        self.alpha0=alpha0\n",
    "\n",
    "    def obj(self,K,y,alpha):\n",
    "        n=len(K)\n",
    "        return np.sum(np.log(1+np.exp(-y*(K@alpha))))/n+self.lmbda*alpha.T@K@alpha/2\n",
    "\n",
    "    def derivative(self,K,y,alpha):\n",
    "        n=len(K)\n",
    "        P=-np.diag(1/(1+np.exp(y*(K@alpha))))\n",
    "        return K@P@y/n+self.lmbda*K@alpha\n",
    "\n",
    "    def train(self,K,y):\n",
    "        ob= lambda alpha : self.obj(K=K,y=y,alpha=alpha)\n",
    "        der=lambda alpha : self.derivative(K=K,y=y,alpha=alpha)\n",
    "        alpha,_,_=fmin_l_bfgs_b(ob, self.alpha0, der, args=(), pgtol=1e-50, factr =1e-50)\n",
    "        self.alpha=alpha\n",
    "    \n",
    "    def fit(self,K):\n",
    "        return K@self.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM():\n",
    "     \"\"\"\n",
    "    Class which train SVM models with Kernels methods takes a dataset with labels in {-1,1} \n",
    "\n",
    "    \"\"\"\n",
    "     def __init__(self,lmbda):\n",
    "          self.lmbda = lmbda\n",
    "\n",
    "     def train(self,K,y):\n",
    "          \"\"\" \n",
    "          We want to solve max mu^T@1 - 1/4*lambda mu^t@diag(y)@K@diag(y)@mu for 0<=mu<=1/n\n",
    "          and then we have :\n",
    "               alpha=diag(y)@mu/2*lambda\n",
    "\n",
    "          We use the package cvxopt wich solve : \n",
    "               min 1/2x^T@P@x + q^T@x for Gx<=h and Ax=b\n",
    "          \"\"\"\n",
    "          n=len(K)\n",
    "\n",
    "          q = -np.ones(n)\n",
    "          P = np.diag(y) @ K @np.diag(y) / (2*self.lmbda)\n",
    "          G = np.concatenate((np.identity(n),-np.identity(n)),axis=0)\n",
    "          h = np.concatenate((np.ones(n)/n , np.zeros(n)),axis=0)[:,None]\n",
    "\n",
    "          mu=solvers.qp(P=matrix(P),q=matrix(q),G=matrix(G),h=matrix(h))['x']\n",
    "          \n",
    "          self.alpha=np.diag(y)@mu / (2*self.lmbda)\n",
    "\n",
    "     \n",
    "     def fit(self,K):\n",
    "          return np.sign(K@self.alpha)\n",
    "     \n",
    "     def pred_prob(self,K):\n",
    "          return K@self.alpha\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelSVC:\n",
    "\n",
    "    \"\"\"\n",
    "    Class which train SVM models with Kernels methods takes a dataset with labels in {-1,1} \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, C, kernel, epsilon = 1e-1):\n",
    "        self.type = 'non-linear'\n",
    "        self.C = C                               \n",
    "        self.kernel = kernel        \n",
    "        self.alpha = None\n",
    "        self.support = None\n",
    "        self.epsilon = epsilon \n",
    "        self.norm_f = None\n",
    "       \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        N = len(y)\n",
    "        K=self.kernel(X,X)\n",
    "        print('kernel computed')\n",
    "        diag=np.diag(y)\n",
    "\n",
    "        # Lagrange dual problem\n",
    "        def loss(alpha):\n",
    "            return  (1/2)*(diag@alpha).T@K@(diag@alpha)-np.sum(alpha) \n",
    "\n",
    "        # Partial derivate of Ld on alpha\n",
    "        def grad_loss(alpha):\n",
    "            return diag@K@diag@alpha-np.ones_like(alpha) \n",
    "\n",
    "\n",
    "        # Constraints on alpha of the shape :\n",
    "        # -  d - C*alpha  = 0\n",
    "        # -  b - A*alpha >= 0\n",
    "\n",
    "        fun_eq = lambda alpha: alpha.T@y  # '''----------------function defining the equality constraint------------------'''        \n",
    "        jac_eq = lambda alpha: y   #'''----------------jacobian wrt alpha of the  equality constraint------------------'''\n",
    "        fun_ineq = lambda alpha: np.concatenate((alpha,self.C-alpha))   # '''---------------function defining the inequality constraint-------------------'''     \n",
    "        jac_ineq = lambda alpha:  np.concatenate((np.identity(len(alpha)),-np.identity(len(alpha)))) # '''---------------jacobian wrt alpha of the  inequality constraint-------------------'''\n",
    "        \n",
    "        constraints = ({'type': 'eq',  'fun': fun_eq, 'jac': jac_eq},\n",
    "                       {'type': 'ineq', \n",
    "                        'fun': fun_ineq , \n",
    "                        'jac': jac_ineq})\n",
    "        print('begin opti :')\n",
    "        optRes = optimize.minimize(fun=lambda alpha: loss(alpha),\n",
    "                                   x0=np.ones(N), \n",
    "                                   method='SLSQP', \n",
    "                                   jac=lambda alpha: grad_loss(alpha), \n",
    "                                   constraints=constraints,tol=self.epsilon)\n",
    "        self.alpha = optRes.x\n",
    "        print('end opti')\n",
    "        ## Attributes\n",
    "        indice_sv=np.where(np.abs(self.alpha)>1e-5)[0]\n",
    "        self.support=X[indice_sv]\n",
    "        self.alpha_support=self.alpha[indice_sv]\n",
    "        self.beta=diag@self.alpha\n",
    "        self.beta_support=self.beta[indice_sv]\n",
    "        margin_indices = np.where((self.alpha > 1e-5) & (self.alpha < self.C-1e-5))[0]\n",
    "        self.margin_points = X[margin_indices]  #'''------------------- A matrix with each row corresponding to a point that falls on the margin ------------------'''\n",
    "        self.b = np.mean(y[indice_sv]-(K@self.beta)[indice_sv])  #''' -----------------offset of the classifier------------------ '''\n",
    "        self.norm_f = self.beta[indice_sv].T@(K@self.beta)[indice_sv]# '''------------------------RKHS norm of the function f ------------------------------'''\n",
    "\n",
    " \n",
    "    def separating_function(self,x):\n",
    "        # Input : matrix x of shape N data points times d dimension\n",
    "        # Output: vector of size N\n",
    "        return self.kernel(x,self.support)@self.beta_support\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict y values in {-1, 1} \"\"\"\n",
    "        d = self.separating_function(X)\n",
    "        return 2 * (d+self.b> 0) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UTILS FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reduce_dataset(X,Y,size):\n",
    "    \"\"\" \n",
    "    Create a dataset of size = 10*size with 10% of each class\n",
    "    \n",
    "    and transform the labels of class k in 1 and the labels of the others classes in -1 \n",
    "    \"\"\"\n",
    "\n",
    "    Y_new=np.array([])\n",
    "    X_new=np.random.randint(2,size=(1,X.shape[1]))\n",
    "\n",
    "    for i in range(len(np.unique(Y))):\n",
    "        ind=np.random.choice(np.array(np.where(Y==i))[0],size=size)\n",
    "        X_new=np.concatenate((X_new,X[ind]))\n",
    "        Y_new=np.concatenate((Y_new,i*np.ones(size)))\n",
    "    X_new=X_new[1:]\n",
    "    ind=np.arange(Y_new.shape[0])\n",
    "    np.random.shuffle(ind)\n",
    "    Y_shuffled=Y_new[ind]\n",
    "    X_shuffled=X_new[ind]\n",
    "    return X_shuffled,Y_shuffled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reduce_dataset_onevsall(X,Y,k,size):\n",
    "    \"\"\" \n",
    "    Create a dataset of size = 18*size with 50% of class k and 50% random classes  \n",
    "    \n",
    "    and transform the labels of class k in 1 and the labels of the others classes in -1 \n",
    "    \"\"\"\n",
    "\n",
    "    Y_new=np.array([])\n",
    "    X_new=np.random.randint(2,size=(1,X.shape[1]))\n",
    "\n",
    "    for i in range(len(np.unique(Y))):\n",
    "        if i == k :\n",
    "            ind=np.random.choice(np.array(np.where(Y==i))[0],size=size*9)\n",
    "        else :\n",
    "            ind=np.random.choice(np.array(np.where(Y==i))[0],size=size)\n",
    "        X_new=np.concatenate((X_new,X[ind]))\n",
    "        if i==k:\n",
    "            Y_new=np.concatenate((Y_new,np.ones(size*9)))\n",
    "        else :\n",
    "            Y_new=np.concatenate((Y_new,-np.ones(size)))\n",
    "            \n",
    "    X_new=X_new[1:]\n",
    "    ind=np.arange(Y_new.shape[0])\n",
    "    np.random.shuffle(ind)\n",
    "    Y_shuffled=Y_new[ind]\n",
    "    X_shuffled=X_new[ind]\n",
    "    return X_shuffled,Y_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred,y_test):\n",
    "    return np.sum(y_pred==y_test)/len(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with the class k=4 and a dataset of size train : 600 and test : 300 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is that the classifier classify well the class k from the others class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y=create_reduce_dataset_onevsall(Xtr,Ytr,k=4,size=50)\n",
    "X_train=X[:600]\n",
    "X_test=X[600:900]\n",
    "Y_train=Y[:600]\n",
    "Y_test=Y[600:900]\n",
    "Big_X=np.concatenate((X_train,X_test),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the Grams Matrix (let's try with polynomial kernel), it takes a lot of time (1min40 for 900x900 kernel) : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = polynomial().kernel(Big_X,X_train)\n",
    "K_train = K[:len(X_train),:len(X_train)]\n",
    "K_test = K[:,len(X_train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_train.shape,K_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try differents models : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RIDGE REG not very appropriated for the problem (classification != regression )\n",
    "classifier=Kernel_ridge_reg(lmbda=0.1)\n",
    "\n",
    "classifier.train(K_train,Y_train) \n",
    "\n",
    "Y_pred = classifier.fit(K_test)\n",
    "\n",
    "accuracy(np.sign(Y_pred),Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic REG \n",
    "classifier=Kernel_logistic_reg(lmbda=0.1,alpha0=np.random.rand(len(X_train)))\n",
    "\n",
    "classifier.train(K_train,Y_train)\n",
    "\n",
    "Y_pred = classifier.fit(K_test.T)\n",
    "\n",
    "accuracy(np.sign(Y_pred),Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier=SVM(lmbda=0.2)\n",
    "\n",
    "classifier.train(K_train,Y_train)\n",
    "\n",
    "Y_pred = classifier.fit(K_test.T)\n",
    "print(classifier.alpha)\n",
    "\n",
    "accuracy(Y_pred[:,0],Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=RBF().kernel\n",
    "classifier=KernelSVC(C=1,kernel=K)\n",
    "classifier.fit(X_train,Y_train)\n",
    "Y_pred=classifier.predict(X_test)\n",
    "accuracy(Y_pred,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to do a loop and attributing an SVM for all classes : \n",
    "ONE VS REST strategy \n",
    "ONE VS ONE strategy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NYSTROM APPROXIMATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/fredhallgren/nystrompca/blob/develop/nystrompca/algorithms/nystrom_KPCA.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NystromKPCA():\n",
    "    \"\"\"\n",
    "    Compute an approximation of kernel with the PCA techniques, \n",
    "    taking m data points, projecting on p directions (p<=m)\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,kernel,p,m) :\n",
    "       self.kernel=kernel\n",
    "       self.m=m\n",
    "       self.p=p\n",
    "       self.alphas=None\n",
    "       self.big_approximated_kernel=None\n",
    "       self.big_approximated_repr=None\n",
    "       self.X_subset=None\n",
    "\n",
    "    def fit_PCA(self, X):\n",
    "        # assigns the vectors\n",
    "        self.X_subset=self.choose_subset(X)\n",
    "\n",
    "        K=self.kernel(self.X_subset,self.X_subset)\n",
    "\n",
    "        #We have to center the kernel\n",
    "        #center = (I-U)K(I-U)\n",
    "        \n",
    "        U=np.ones(K.shape)\n",
    "        I=np.eye(K.shape[0])\n",
    "        K=(I-U)@K@(I-U)\n",
    "        \n",
    "        valp,vectp=np.linalg.eigh(K)\n",
    "\n",
    "        #we take the last r eingenvalues\n",
    "        lmbda=valp[-self.p:]\n",
    "        inverse=1/np.sqrt(lmbda)\n",
    "\n",
    "        self.alphas=(inverse*vectp[:,-self.p:]).T\n",
    "    \n",
    "\n",
    "    def approximated_repr(self,x):\n",
    "        return self.alphas@self.kernel(self.X_subset,x)\n",
    "\n",
    "    def appro_kernel(self,X,Y):\n",
    "        repr_X=self.alphas@self.kernel(self.X_subset,X)\n",
    "        repr_Y=self.alphas@self.kernel(self.X_subset,Y)\n",
    "        return repr_X.T@repr_Y\n",
    "\n",
    "    def choose_subset(self,X):\n",
    "        self.n=X.shape[0]\n",
    "        set=np.random.choice(self.n,self.m)\n",
    "        self.X_subset=X[set]\n",
    "        return set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_onevsall(Y,k):\n",
    "    \"\"\"  \n",
    "    Transform the labels of class k in 1 and the labels of the others classes in -1 \n",
    "    \"\"\"\n",
    "    Y_onevall=-np.ones_like(Y)\n",
    "    Y_onevall[Y==k]=1\n",
    "    return Y_onevall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_onevall=create_dataset_onevsall(Ytr,1)\n",
    "Xtr[:100,:].shape,Y_onevall[:100].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel=RBF().kernel\n",
    "classifier=KernelSVC(C=1,kernel=kernel)\n",
    "classifier.fit(Xtr[:2000,:],Y_onevall[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(classifier.separating_function(Xtr[:2000,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=classifier.predict(Xtr[2000:3000,:])==Y_onevall[2000:3000]\n",
    "np.sum(l)/len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONE vs REST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "SVMModels = []\n",
    "\n",
    "X_reduce,Y_reduce = create_reduce_dataset(Xtr,Ytr,150)\n",
    "\n",
    "# Ãtape 4-8: GÃ©nÃ©rer N modÃ¨les de classe binaire\n",
    "for j in tqdm(range(10)):\n",
    "\n",
    "    Y = create_dataset_onevsall(Y_reduce,k=j)\n",
    "    \n",
    "    kernel=RBF().kernel\n",
    "    classifier=KernelSVC(C=1,kernel=kernel)\n",
    "    \n",
    "    classifier.fit(X_reduce,Y_reduce)\n",
    "    \n",
    "    # Stocker le modÃ¨le SVM entraÃ®nÃ©\n",
    "    SVMModels.append(classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test,y_test = create_reduce_dataset(Xtr,Ytr,20)\n",
    "x_test=X_reduce\n",
    "\n",
    "# Initialisation d'un dictionnaire pour stocker les scores pour chaque classe\n",
    "scores = np.zeros((10,x_test.shape[0]))\n",
    "\n",
    "# Ãtape 9-12: Calculer les scores pour chaque classe\n",
    "for j, svm_model in enumerate(SVMModels):\n",
    "    # PrÃ©dire le score pour la classe actuelle\n",
    "    score = svm_model.separating_function(x_test)\n",
    "    \n",
    "    # Stocker le score pour la classe actuelle\n",
    "    scores[j] = score\n",
    "\n",
    "# Ãtape 13: Attribuer Ã  chaque observation la classe avec le score le plus Ã©levÃ©\n",
    "final_classes = np.argmax(scores,axis=0)+1\n",
    "accuracy(final_classes,Y_reduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVMModels[0].alpha[116]\n",
    "SVMModels[0].beta[421]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APPRO DE LA DIM PAR PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelPCA:\n",
    "    \n",
    "    def __init__(self,kernel, r=2):                             \n",
    "        self.kernel = kernel          # <---\n",
    "        self.alpha = None # Matrix of shape N times d representing the d eingenvectors alpha corresp\n",
    "        self.lmbda = None # Vector of size d representing the top d eingenvalues\n",
    "        self.support = None # Data points where the features are evaluated\n",
    "        self.r =r ## Number of principal components\n",
    "\n",
    "    def compute_PCA(self, X):\n",
    "        # assigns the vectors\n",
    "        \n",
    "        self.support = X\n",
    "        K=self.kernel(X,X)\n",
    "\n",
    "        #We have to center the kernel\n",
    "        #center = (I-U)K(I-U)\n",
    "        \n",
    "        U=np.ones(K.shape)\n",
    "        I=np.eye(K.shape[0])\n",
    "        K=(I-U)@K@(I-U)\n",
    "        \n",
    "        valp,vectp=np.linalg.eigh(K)\n",
    "\n",
    "        #we take the last r eingenvalues\n",
    "        self.lmbda=valp[-self.r:]\n",
    "        inverse=1/np.sqrt(self.lmbda)\n",
    "\n",
    "        self.alpha = inverse*vectp[:,-self.r:]\n",
    "\n",
    "        \n",
    "        #constraints = ({})\n",
    "        # Maximize by minimizing the opposite\n",
    "        \n",
    "    def transform(self,x):\n",
    "        # Input : matrix x of shape N data points times d dimension\n",
    "        # Output: vector of size N\n",
    "        K=self.kernel(x,self.support)\n",
    "        return K@self.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kernel=RBF().kernel\n",
    "PCA=KernelPCA(kernel=kernel,r=1000)\n",
    "PCA.compute_PCA(Xtr) # We choose the most dim of X\n",
    "X=PCA.transform(Xtr)\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "\n",
    "#classifier.fit(Xtr[:1000,0:10],Y[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = create_dataset_onevsall(Ytr,k=3)\n",
    "\n",
    "kernel=RBF().kernel\n",
    "classifier=KernelSVC(C=1,kernel=kernel)\n",
    "\n",
    "classifier.fit(Xtr,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "SVMModels = []\n",
    "\n",
    "# Ãtape 4-8: GÃ©nÃ©rer N modÃ¨les de classe binaire\n",
    "for j in tqdm(range(10)):\n",
    "\n",
    "    Y = create_dataset_onevsall(Ytr,k=j)\n",
    "    \n",
    "    kernel=RBF().kernel\n",
    "    classifier=KernelSVC(C=1,kernel=kernel)\n",
    "    \n",
    "    classifier.fit(X,Y)\n",
    "    \n",
    "    # Stocker le modÃ¨le SVM entraÃ®nÃ©\n",
    "    SVMModels.append(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Logistic reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kernel_logistic_reg():\n",
    "    \"\"\"\n",
    "    Class which comput the solution to the Kernel logistic Regression with regularization parameter lambda (lmbda)\n",
    "\n",
    "    ----> We have to solve the smooth convex opti problem : \n",
    "            alpha=argmin 1/n sum_i=1^n logisitic(y_i[Kalpha]_i)+lambda/2 *alpha^T@K@alpha\n",
    "\n",
    "          We solve this with the L-FBGS form scipy\n",
    "\n",
    "          and then : \n",
    "            pred=K@alpha\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,lmbda,alpha0):\n",
    "        self.lmbda=lmbda\n",
    "        self.alpha0=alpha0\n",
    "\n",
    "    def obj(self,K,y,alpha):\n",
    "        n=len(K)\n",
    "        return np.sum(np.log(1+np.exp(-y*(K@alpha))))/n+self.lmbda*alpha.T@K@alpha/2\n",
    "    \n",
    "    def sigmoid(self,u):\n",
    "        return 1 / (1 + np.exp(-u))\n",
    "\n",
    "    def logistic(self,u):\n",
    "        return np.log(1 + np.exp(-u))\n",
    "\n",
    "\n",
    "    def logistic_prime(self,u):\n",
    "        return -self.sigmoid(-u)\n",
    "\n",
    "\n",
    "    def logistic_prime2(self,u):\n",
    "        return self.sigmoid(u) * self.sigmoid(-u)\n",
    "\n",
    "\n",
    "    def derivative(self,K,y,alpha):\n",
    "        n=len(K)\n",
    "        P=-np.diag(1/(1+np.exp(y*(K@alpha))))\n",
    "        return K@P@y/n+self.lmbda*K@alpha\n",
    "    \n",
    "    def second_derivative(self,K,y,alpha):\n",
    "       n=len(K)\n",
    "       W=np.diag(self.logistic_prime2(y*(K@alpha)))\n",
    "       return (1/n)* K@W@K + self.lmbda*K\n",
    "\n",
    "    def train(self,K,y):\n",
    "        ob= lambda alpha : self.obj(K=K,y=y,alpha=alpha)\n",
    "        der=lambda alpha : self.derivative(K=K,y=y,alpha=alpha)\n",
    "        secder=lambda alpha : self.second_derivative(K=K,y=y,alpha=alpha)\n",
    "        \n",
    "        optRes = optimize.minimize(fun=ob,\n",
    "                                   x0=np.ones(len(K)), \n",
    "                                   method='SLSQP', \n",
    "                                   jac=der,hess=secder\n",
    "                                   ,tol=1e-5)\n",
    "        self.alpha = optRes.x\n",
    "    \n",
    "    def fit(self,K):\n",
    "        return self.sigmoid(K@self.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelLogisticRegression:\n",
    "\n",
    "    def __init__(self, kernel, reg_param=0, epsilon=1e-8):\n",
    "        self.alpha = None\n",
    "        self.reg_param = reg_param\n",
    "        self.beta = None\n",
    "        self.kernel = kernel\n",
    "        self.eps = epsilon\n",
    "        self.support=None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        N = X.shape[0]\n",
    "        self.support=X\n",
    "        #features_X = self.kernel.fit_subtree(X)\n",
    "        k = self.kernel(X,X)\n",
    "        alpha = np.zeros(N)\n",
    "        alpha_old = alpha + np.inf\n",
    "        sig = np.vectorize(sigmoid)\n",
    "        logpp = np.vectorize(logistic_prime2)\n",
    "        i=0\n",
    "        while (np.abs(alpha - alpha_old) > self.eps).any():\n",
    "            # Update coefs\n",
    "            m = k @ alpha\n",
    "            W = np.diag(logpp(y * m))\n",
    "            z = m + y / sig(y * m)\n",
    "\n",
    "            # Solve Weighted KRR\n",
    "\n",
    "            sqrt_W = np.sqrt(W)\n",
    "\n",
    "            alpha_old = alpha\n",
    "            alpha = sqrt_W @ np.linalg.inv(\n",
    "                sqrt_W @ k @ sqrt_W + N * self.reg_param * np.eye(N)\n",
    "            ) @ sqrt_W @ z\n",
    "            print(f'{i}Ã¨me iteration, epsilon :{np.max(np.abs(alpha - alpha_old))}')\n",
    "            i+=1\n",
    "            if i ==10 :\n",
    "                break\n",
    "\n",
    "        self.alpha = alpha\n",
    "\n",
    "        return sigmoid(np.einsum('i, ij->j', self.alpha, k))\n",
    "\n",
    "    def predict(self, X):\n",
    "        # features_pred = self.kernel.predict(X)\n",
    "        # print(features_pred.shape, self.features.shape)\n",
    "        K_Xx = self.kernel(X, self.support)\n",
    "        predictions = sigmoid(np.einsum('i, ij->j', self.alpha, K_Xx.T))\n",
    "        return predictions  # *-1 because inverted prediction on 1 et -1\n",
    "\n",
    "\n",
    "def logistic(u):\n",
    "    return np.log(1 + np.exp(-u))\n",
    "\n",
    "\n",
    "def logistic_prime(u):\n",
    "    return -sigmoid(-u)\n",
    "\n",
    "\n",
    "def logistic_prime2(u):\n",
    "    return sigmoid(u) * sigmoid(-u)\n",
    "\n",
    "\n",
    "def sigmoid(u):\n",
    "    return 1 / (1 + np.exp(-u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = create_dataset_onevsall(Ytr,k=1)\n",
    "    \n",
    "kernel=RBF().kernel\n",
    "classifier=KernelLogisticRegression(kernel=kernel)\n",
    "classifier.fit(Xtr,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = create_dataset_onevsall(Ytr,k=1)\n",
    "    \n",
    "kernel=RBF().kernel\n",
    "classifier=KernelLogisticRegression(kernel=kernel)\n",
    "classifier.fit(Xtr,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=np.zeros(5000)\n",
    "pred[classifier.predict(Xtr)>0.5]=1\n",
    "pred[classifier.predict(Xtr)<=0.5]=-1\n",
    "accuracy(pred,Ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = create_dataset_onevsall(Ytr,k=1)\n",
    "    \n",
    "kernel=RBF().kernel\n",
    "classifier=Kernel_logistic_reg(0.01,np.zeros(len(Xtr)))\n",
    "K=kernel(Xtr,Xtr)\n",
    "classifier.train(K,Ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "LogModels = []\n",
    "\n",
    "# Ãtape 4-8: GÃ©nÃ©rer N modÃ¨les de classe binaire\n",
    "for j in tqdm(range(10)):\n",
    "\n",
    "    Y = create_dataset_onevsall(Ytr,k=j)\n",
    "    \n",
    "    kernel=RBF().kernel\n",
    "    classifier=KernelLogisticRegression(kernel,reg_param=0.001,epsilon=1e-6)\n",
    "    \n",
    "    classifier.fit(Xtr,Y)\n",
    "    \n",
    "    # Stocker le modÃ¨le SVM entraÃ®nÃ©\n",
    "    LogModels.append(classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = Xte\n",
    "\n",
    "# Initialisation d'un dictionnaire pour stocker les scores pour chaque classe\n",
    "scores = np.zeros((10,x_test.shape[0]))\n",
    "\n",
    "# Ãtape 9-12: Calculer les scores pour chaque classe\n",
    "for j, log_model in enumerate(LogModels):\n",
    "    # PrÃ©dire le score pour la classe actuelle\n",
    "    score = log_model.predict(x_test)\n",
    "    \n",
    "    # Stocker le score pour la classe actuelle\n",
    "    scores[j] = score\n",
    "\n",
    "# Ãtape 13: Attribuer Ã  chaque observation la classe avec le score le plus Ã©levÃ©\n",
    "final_classes = np.argmax(scores,axis=0)+1\n",
    "final_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_classes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CrÃ©er un DataFrame Ã  partir des prÃ©dictions\n",
    "# Remplacez 'predictions' par vos donnÃ©es de prÃ©diction\n",
    "df = pd.DataFrame({\n",
    "    'Id': range(1, 2001),  # CrÃ©er une colonne d'ID de 1 Ã  2000\n",
    "    'Prediction': final_classes  # Utilisez vos propres rÃ©sultats de prÃ©diction ici\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('submission2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One vs one "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KRR \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelRR:\n",
    "    \n",
    "    def __init__(self,kernel,lmbda):\n",
    "        self.lmbda = lmbda                    \n",
    "        self.kernel = kernel    \n",
    "        self.alpha = None \n",
    "        self.b = None\n",
    "        self.support = None\n",
    "        self.type='ridge'\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        N=len(y)\n",
    "        self.support = X\n",
    "        ones=np.ones((N,1))\n",
    "        K=self.kernel(X,X)\n",
    "        K_prime=np.block([[K, ones], [ones.T, np.ones((1, 1))]])\n",
    "        y_prime=np.append(y,[0])\n",
    "\n",
    "        mat=K_prime+(N/2)*self.lmbda*np.identity(N+1)\n",
    "\n",
    "        alpha_prime=np.linalg.solve(mat,y_prime)  \n",
    "        self.alpha = alpha_prime[:-1]  \n",
    "        self.b=alpha_prime[-1]     \n",
    "        \n",
    "    ### Implementation of the separting function $f$ \n",
    "    def regression_function(self,x):\n",
    "        # Input : matrix x of shape N data points times d dimension\n",
    "        # Output: vector of size N\n",
    "        K=self.kernel(x,self.support)\n",
    "        return K@self.alpha\n",
    "\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict y values in {-1, 1} \"\"\"\n",
    "        return self.regression_function(X)+self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 3072)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma =0.0001\n",
    "lmbda= 0.01\n",
    "kernel=polynomial(d=5).kernel\n",
    "ridge=KernelRR(kernel,lmbda=lmbda)\n",
    "ridge.fit(Xtr[:4500],Ytr[:4500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.82071437,  1.99011693,  8.95661452,  4.93874889,  1.83716675,\n",
       "        3.59391532, -0.63423097,  4.91184532,  4.27726282,  8.81121122,\n",
       "       -3.64590529, -3.64537123, -3.65213901, -3.63929281, -3.62814046,\n",
       "       -3.67154309, -3.60491434, -3.64469469, -3.64716204, -3.63841473])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge.predict(Xtr)[4490:4510]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 0, 4, 3, 9, 0, 2, 4, 7, 9, 8, 8, 6, 7, 9, 0, 7, 4, 1, 8, 2, 6,\n",
       "       5, 7, 5, 8, 5, 2, 8, 0, 7, 1, 8, 7, 2, 7, 9, 8, 9, 5, 9, 7, 5, 7,\n",
       "       4, 6, 5, 4, 7, 8, 7, 2, 4, 4, 7, 1, 6, 0, 4, 0, 0, 9, 3, 1, 9, 8,\n",
       "       2, 9, 8, 2, 1, 5, 3, 9, 6, 4, 1, 1, 3, 8, 7, 2, 4, 2, 1, 0, 0, 1,\n",
       "       2, 5, 3, 6, 3, 1, 5, 2, 4, 6, 8, 2, 6, 9, 7, 4, 5, 9, 6, 4, 2, 9,\n",
       "       0, 5, 9, 1, 6, 3, 2, 2, 2, 1, 4, 7, 4, 2, 0, 0, 9, 1, 8, 5, 9, 9,\n",
       "       0, 2, 6, 7, 5, 0, 7, 0, 2, 8, 9, 2, 9, 5, 6, 3, 3, 8, 4, 6, 8, 1,\n",
       "       5, 3, 1, 8, 5, 6, 0, 9, 8, 7, 7, 3, 5, 1, 2, 7, 6, 8, 8, 0, 8, 6,\n",
       "       5, 6, 6, 1, 9, 7, 4, 9, 3, 2, 6, 3, 4, 6, 7, 9, 8, 6, 0, 3, 4, 3,\n",
       "       4, 1, 5, 7, 0, 3, 2, 0, 7, 3, 0, 5, 6, 8, 4, 8, 9, 6, 7, 0, 5, 0,\n",
       "       5, 8, 0, 6, 8, 7, 3, 7, 7, 0, 4, 4, 4, 3, 3, 3, 8, 0, 4, 2, 3, 3,\n",
       "       7, 4, 1, 9, 0, 8, 8, 2, 7, 7, 6, 6, 0, 5, 3, 9, 9, 5, 8, 4, 9, 0,\n",
       "       1, 8, 3, 7, 2, 8, 3, 7, 2, 5, 4, 9, 3, 2, 1, 5, 2, 0, 4, 8, 4, 7,\n",
       "       4, 2, 9, 2, 3, 7, 9, 5, 5, 4, 6, 6, 6, 7, 9, 5, 8, 4, 9, 0, 3, 2,\n",
       "       4, 0, 3, 8, 3, 5, 2, 3, 7, 6, 6, 0, 5, 5, 1, 8, 0, 1, 7, 8, 0, 9,\n",
       "       9, 8, 8, 0, 2, 1, 6, 6, 2, 4, 5, 7, 6, 7, 2, 5, 6, 5, 2, 8, 4, 5,\n",
       "       1, 8, 2, 0, 7, 3, 2, 1, 3, 7, 1, 2, 4, 6, 8, 1, 1, 6, 6, 5, 4, 8,\n",
       "       5, 7, 3, 4, 3, 5, 9, 6, 6, 7, 1, 7, 3, 7, 3, 6, 4, 1, 3, 0, 2, 2,\n",
       "       9, 2, 7, 8, 5, 2, 5, 8, 0, 4, 3, 1, 7, 5])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ytr[4590:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
