{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy.optimize import fmin_l_bfgs_b \n",
    "from cvxopt import matrix, solvers\n",
    "import pickle as pkl\n",
    "from scipy import optimize\n",
    "from scipy.linalg import cho_factor, cho_solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr = np.array(pd.read_csv('data_image/Xtr.csv',header=None,sep=',',usecols=range(3072))) \n",
    "Xte = np.array(pd.read_csv('data_image/Xte.csv',header=None,sep=',',usecols=range(3072))) \n",
    "Ytr = np.array(pd.read_csv('data_image/Ytr.csv',sep=',',usecols=[1])).squeeze() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIZUALISATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images_grid(data, nrows, ncols):\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols*2, nrows*2))\n",
    "    random=np.random.choice(data.shape[0],size=nrows*ncols)\n",
    "    #data=(data-np.min(data))/(np.max(data)-np.min(data))\n",
    "    for j, ax in enumerate(axes.flat):\n",
    "        i=random[j]\n",
    "        if i < data.shape[0]:\n",
    "            image_data = data[i, :]\n",
    "            \n",
    "            # Normaliser les donnÃ©es dans l'intervalle [0, 1]\n",
    "            # min_val = image_data.min()\n",
    "            # max_val = image_data.max()\n",
    "            # print(min_val,max_val)\n",
    "            # image_data = (image_data - min_val) / (max_val - min_val)\n",
    "            # print(image_data.max())\n",
    "            \n",
    "            red_channel = image_data[:1024].reshape((32, 32))\n",
    "            red_channel=(red_channel-red_channel.min())/(red_channel.max()-red_channel.min())\n",
    "            green_channel = image_data[1024:2048].reshape((32, 32))\n",
    "            green_channel=(green_channel-green_channel.min())/(green_channel.max()-green_channel.min())\n",
    "            blue_channel = image_data[2048:].reshape((32, 32))\n",
    "            blue_channel=(blue_channel-blue_channel.min())/(blue_channel.max()-blue_channel.min())\n",
    "            image = np.stack((red_channel, green_channel, blue_channel), axis=-1)\n",
    "\n",
    "            ax.imshow(image)\n",
    "            ax.axis('off')\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "plot_images_grid(Xtr,2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KERNELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some kernels :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBF():\n",
    "    \"\"\"\n",
    "    Compute the matrix of the Gaussian Kernel : \n",
    "    \n",
    "    (K)_ij=K(X_i,Y_j)=exp( 1/2*sigma^2 * ||X_i-Y_j||^2 ) the Gaussian Kernel evaluated between the ith data and jth data\n",
    "\n",
    "    Parameters : \n",
    "    X : 2d array size (n,p) \n",
    "        the Data matrix with n the number of data and p the size of data\n",
    "    Y : 2d array size (q,p) \n",
    "        the Data matrix with q the number of data and p the size of data\n",
    "    sigma : float \n",
    "             Variance of the GaussianKernel \n",
    "\n",
    "    Outputs :\n",
    "    2d array size (n,q)\n",
    "    Matrix of the Gaussian Kernel\n",
    "    \"\"\"   \n",
    "    def __init__(self, sigma=1.):\n",
    "        self.sigma = sigma  ## the variance of the kernel\n",
    "    def kernel(self,X,Y):\n",
    "        ## Input vectors X and Y of shape Nxd and Mxd\n",
    "        diff2 = np.sum(X**2, axis=1)[:, None] + np.sum(Y**2, axis=1)[None, :] - 2 * np.dot(X, Y.T)\n",
    "        return  np.exp(-diff2/(2*self.sigma**2)) ## Matrix of shape NxM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class polynomial():\n",
    "    \"\"\"\n",
    "    Compute the matrix of the Polynomial Kernel : \n",
    "    \n",
    "    (K)_ij=K(X_i,Y_j)=(<X_i,Y_j>)^d the Polynomial Kernel of degree d evaluated between the ith data and jth data\n",
    "\n",
    "    Parameters : \n",
    "    X : 2d array size (n,p) \n",
    "        the Data matrix with n the number of data and p the size of data\n",
    "    Y : 2d array size (q,p) \n",
    "        the Data matrix with q the number of data and p the size of data\n",
    "    d : Integer\n",
    "            Degree of the polynomial kernel\n",
    "\n",
    "    Outputs :\n",
    "    2d array size (n,q)\n",
    "    Matrix of the Gaussian Kernel\n",
    "    \"\"\"\n",
    "    def __init__(self,d=2):\n",
    "        self.d=d\n",
    "\n",
    "    def kernel(self,X,Y):\n",
    "        return np.dot(X,Y.T) ** self.d\n",
    "        # X_intercept = np.concatenate((X,np.ones(X.shape[0]).reshape(-1,1)), axis=1)\n",
    "        # Y_intercept= np.concatenate((Y,np.ones(Y.shape[0]).reshape(-1,1)), axis=1)\n",
    "        # return np.sum(X_intercept * Y_intercept[:,None,:], axis=-1) ** self.d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kernel_ridge_reg():\n",
    "    \"\"\"\n",
    "    Class which comput the solution to the Kernel Ridge Regression with regularization parameter lambda (lmbda)\n",
    "\n",
    "    ----> We have a close form for the solution : \n",
    "            alpha=(K+lambda*n*I)^-1y\n",
    "            pred=K@alpha\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,lmbda):\n",
    "        self.lmbda=lmbda\n",
    "\n",
    "    def train(self,K,y):\n",
    "        mat=K+self.lmbda*K.shape[0]*np.identity(K.shape[0])\n",
    "        self.alpha=scipy.linalg.solve(mat,y)\n",
    "    \n",
    "    def fit(self,K):\n",
    "        print(self.alpha.shape,K.shape)\n",
    "        return self.alpha@K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kernel_logistic_reg():\n",
    "    \"\"\"\n",
    "    Class which comput the solution to the Kernel logistic Regression with regularization parameter lambda (lmbda)\n",
    "\n",
    "    ----> We have to solve the smooth convex opti problem : \n",
    "            alpha=argmin 1/n sum_i=1^n logisitic(y_i[Kalpha]_i)+lambda/2 *alpha^T@K@alpha\n",
    "\n",
    "          We solve this with the L-FBGS form scipy\n",
    "\n",
    "          and then : \n",
    "            pred=K@alpha\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,lmbda,alpha0):\n",
    "        self.lmbda=lmbda\n",
    "        self.alpha0=alpha0\n",
    "\n",
    "    def obj(self,K,y,alpha):\n",
    "        n=len(K)\n",
    "        return np.sum(np.log(1+np.exp(-y*(K@alpha))))/n+self.lmbda*alpha.T@K@alpha/2\n",
    "\n",
    "    def derivative(self,K,y,alpha):\n",
    "        n=len(K)\n",
    "        P=-np.diag(1/(1+np.exp(y*(K@alpha))))\n",
    "        return K@P@y/n+self.lmbda*K@alpha\n",
    "\n",
    "    def train(self,K,y):\n",
    "        ob= lambda alpha : self.obj(K=K,y=y,alpha=alpha)\n",
    "        der=lambda alpha : self.derivative(K=K,y=y,alpha=alpha)\n",
    "        alpha,_,_=fmin_l_bfgs_b(ob, self.alpha0, der, args=(), pgtol=1e-50, factr =1e-50)\n",
    "        self.alpha=alpha\n",
    "    \n",
    "    def fit(self,K):\n",
    "        return K@self.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM():\n",
    "     \"\"\"\n",
    "    Class which train SVM models with Kernels methods takes a dataset with labels in {-1,1} \n",
    "\n",
    "    \"\"\"\n",
    "     def __init__(self,lmbda):\n",
    "          self.lmbda = lmbda\n",
    "\n",
    "     def train(self,K,y):\n",
    "          \"\"\" \n",
    "          We want to solve max mu^T@1 - 1/4*lambda mu^t@diag(y)@K@diag(y)@mu for 0<=mu<=1/n\n",
    "          and then we have :\n",
    "               alpha=diag(y)@mu/2*lambda\n",
    "\n",
    "          We use the package cvxopt wich solve : \n",
    "               min 1/2x^T@P@x + q^T@x for Gx<=h and Ax=b\n",
    "          \"\"\"\n",
    "          n=len(K)\n",
    "\n",
    "          q = -np.ones(n)\n",
    "          P = np.diag(y) @ K @np.diag(y) / (2*self.lmbda)\n",
    "          G = np.concatenate((np.identity(n),-np.identity(n)),axis=0)\n",
    "          h = np.concatenate((np.ones(n)/n , np.zeros(n)),axis=0)[:,None]\n",
    "\n",
    "          mu=solvers.qp(P=matrix(P),q=matrix(q),G=matrix(G),h=matrix(h))['x']\n",
    "          \n",
    "          self.alpha=np.diag(y)@mu / (2*self.lmbda)\n",
    "\n",
    "     \n",
    "     def fit(self,K):\n",
    "          return np.sign(K@self.alpha)\n",
    "     \n",
    "     def pred_prob(self,K):\n",
    "          return K@self.alpha\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelSVC:\n",
    "\n",
    "    \"\"\"\n",
    "    Class which train SVM models with Kernels methods takes a dataset with labels in {-1,1} \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, C, kernel, epsilon = 1e-1):\n",
    "        self.type = 'non-linear'\n",
    "        self.C = C                               \n",
    "        self.kernel = kernel        \n",
    "        self.alpha = None\n",
    "        self.support = None\n",
    "        self.epsilon = epsilon \n",
    "        self.norm_f = None\n",
    "       \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        N = len(y)\n",
    "        K=self.kernel(X,X)\n",
    "        print('kernel computed')\n",
    "        diag=np.diag(y)\n",
    "\n",
    "        # Lagrange dual problem\n",
    "        def loss(alpha):\n",
    "            return  (1/2)*(diag@alpha).T@K@(diag@alpha)-np.sum(alpha) \n",
    "\n",
    "        # Partial derivate of Ld on alpha\n",
    "        def grad_loss(alpha):\n",
    "            return diag@K@diag@alpha-np.ones_like(alpha) \n",
    "\n",
    "\n",
    "        # Constraints on alpha of the shape :\n",
    "        # -  d - C*alpha  = 0\n",
    "        # -  b - A*alpha >= 0\n",
    "\n",
    "        fun_eq = lambda alpha: alpha.T@y  # '''----------------function defining the equality constraint------------------'''        \n",
    "        jac_eq = lambda alpha: y   #'''----------------jacobian wrt alpha of the  equality constraint------------------'''\n",
    "        fun_ineq = lambda alpha: np.concatenate((alpha,self.C-alpha))   # '''---------------function defining the inequality constraint-------------------'''     \n",
    "        jac_ineq = lambda alpha:  np.concatenate((np.identity(len(alpha)),-np.identity(len(alpha)))) # '''---------------jacobian wrt alpha of the  inequality constraint-------------------'''\n",
    "        \n",
    "        constraints = ({'type': 'eq',  'fun': fun_eq, 'jac': jac_eq},\n",
    "                       {'type': 'ineq', \n",
    "                        'fun': fun_ineq , \n",
    "                        'jac': jac_ineq})\n",
    "        print('begin opti :')\n",
    "        optRes = optimize.minimize(fun=lambda alpha: loss(alpha),\n",
    "                                   x0=np.ones(N), \n",
    "                                   method='SLSQP', \n",
    "                                   jac=lambda alpha: grad_loss(alpha), \n",
    "                                   constraints=constraints,tol=self.epsilon)\n",
    "        self.alpha = optRes.x\n",
    "        print('end opti')\n",
    "        ## Attributes\n",
    "        print('begin operation')\n",
    "        indice_sv=np.where(np.abs(self.alpha)>1e-5)[0]\n",
    "        self.support=X[indice_sv]\n",
    "        self.alpha_support=self.alpha[indice_sv]\n",
    "        print('1')\n",
    "        self.beta=diag@self.alpha\n",
    "        self.beta_support=self.beta[indice_sv]\n",
    "        print('2')\n",
    "        margin_indices = np.where((self.alpha > 1e-5) & (self.alpha < self.C-1e-5))[0]\n",
    "        self.margin_points = X[margin_indices]  #'''------------------- A matrix with each row corresponding to a point that falls on the margin ------------------'''\n",
    "        print('3')\n",
    "        self.b = np.mean(y[indice_sv]-(K@self.beta)[indice_sv])  #''' -----------------offset of the classifier------------------ '''\n",
    "        print('4')\n",
    "        self.norm_f = self.beta[indice_sv].T@(K@self.beta)[indice_sv]# '''------------------------RKHS norm of the function f ------------------------------'''\n",
    "\n",
    " \n",
    "    def separating_function(self,x):\n",
    "        # Input : matrix x of shape N data points times d dimension\n",
    "        # Output: vector of size N\n",
    "        return self.kernel(x,self.support)@self.beta_support\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict y values in {-1, 1} \"\"\"\n",
    "        d = self.separating_function(X)\n",
    "        return 2 * (d+self.b> 0) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UTILS FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reduce_dataset(X,Y,size):\n",
    "    \"\"\" \n",
    "    Create a dataset of size = 10*size with 10% of each class\n",
    "    \n",
    "    and transform the labels of class k in 1 and the labels of the others classes in -1 \n",
    "    \"\"\"\n",
    "\n",
    "    Y_new=np.array([])\n",
    "    X_new=np.random.randint(2,size=(1,X.shape[1]))\n",
    "\n",
    "    for i in range(len(np.unique(Y))):\n",
    "        ind=np.random.choice(np.array(np.where(Y==i))[0],size=size)\n",
    "        X_new=np.concatenate((X_new,X[ind]))\n",
    "        Y_new=np.concatenate((Y_new,i*np.ones(size)))\n",
    "    X_new=X_new[1:]\n",
    "    ind=np.arange(Y_new.shape[0])\n",
    "    np.random.shuffle(ind)\n",
    "    Y_shuffled=Y_new[ind]\n",
    "    X_shuffled=X_new[ind]\n",
    "    return X_shuffled,Y_shuffled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reduce_dataset_onevsall(X,Y,k,size):\n",
    "    \"\"\" \n",
    "    Create a dataset of size = 18*size with 50% of class k and 50% random classes  \n",
    "    \n",
    "    and transform the labels of class k in 1 and the labels of the others classes in -1 \n",
    "    \"\"\"\n",
    "\n",
    "    Y_new=np.array([])\n",
    "    X_new=np.random.randint(2,size=(1,X.shape[1]))\n",
    "\n",
    "    for i in range(len(np.unique(Y))):\n",
    "        if i == k :\n",
    "            ind=np.random.choice(np.array(np.where(Y==i))[0],size=size*9)\n",
    "        else :\n",
    "            ind=np.random.choice(np.array(np.where(Y==i))[0],size=size)\n",
    "        X_new=np.concatenate((X_new,X[ind]))\n",
    "        if i==k:\n",
    "            Y_new=np.concatenate((Y_new,np.ones(size*9)))\n",
    "        else :\n",
    "            Y_new=np.concatenate((Y_new,-np.ones(size)))\n",
    "            \n",
    "    X_new=X_new[1:]\n",
    "    ind=np.arange(Y_new.shape[0])\n",
    "    np.random.shuffle(ind)\n",
    "    Y_shuffled=Y_new[ind]\n",
    "    X_shuffled=X_new[ind]\n",
    "    return X_shuffled,Y_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred,y_test):\n",
    "    return np.sum(y_pred==y_test)/len(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with the class k=4 and a dataset of size train : 600 and test : 300 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is that the classifier classify well the class k from the others class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y=create_reduce_dataset_onevsall(Xtr,Ytr,k=4,size=50)\n",
    "X_train=X[:600]\n",
    "X_test=X[600:900]\n",
    "Y_train=Y[:600]\n",
    "Y_test=Y[600:900]\n",
    "Big_X=np.concatenate((X_train,X_test),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the Grams Matrix (let's try with polynomial kernel), it takes a lot of time (1min40 for 900x900 kernel) : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = polynomial().kernel(Big_X,X_train)\n",
    "K_train = K[:len(X_train),:len(X_train)]\n",
    "K_test = K[:,len(X_train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_train.shape,K_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try differents models : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RIDGE REG not very appropriated for the problem (classification != regression )\n",
    "classifier=Kernel_ridge_reg(lmbda=0.1)\n",
    "\n",
    "classifier.train(K_train,Y_train) \n",
    "\n",
    "Y_pred = classifier.fit(K_test)\n",
    "\n",
    "accuracy(np.sign(Y_pred),Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic REG \n",
    "classifier=Kernel_logistic_reg(lmbda=0.1,alpha0=np.random.rand(len(X_train)))\n",
    "\n",
    "classifier.train(K_train,Y_train)\n",
    "\n",
    "Y_pred = classifier.fit(K_test.T)\n",
    "\n",
    "accuracy(np.sign(Y_pred),Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier=SVM(lmbda=0.2)\n",
    "\n",
    "classifier.train(K_train,Y_train)\n",
    "\n",
    "Y_pred = classifier.fit(K_test.T)\n",
    "print(classifier.alpha)\n",
    "\n",
    "accuracy(Y_pred[:,0],Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=RBF().kernel\n",
    "classifier=KernelSVC(C=1,kernel=K)\n",
    "classifier.fit(X_train,Y_train)\n",
    "Y_pred=classifier.predict(X_test)\n",
    "accuracy(Y_pred,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to do a loop and attributing an SVM for all classes : \n",
    "ONE VS REST strategy \n",
    "ONE VS ONE strategy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NYSTROM APPROXIMATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/fredhallgren/nystrompca/blob/develop/nystrompca/algorithms/nystrom_KPCA.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NystromKPCA():\n",
    "    \"\"\"\n",
    "    Compute an approximation of kernel with the PCA techniques, \n",
    "    taking m data points, projecting on p directions (p<=m)\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,kernel,p,m) :\n",
    "       self.kernel=kernel\n",
    "       self.m=m\n",
    "       self.p=p\n",
    "       self.alphas=None\n",
    "       self.big_approximated_kernel=None\n",
    "       self.big_approximated_repr=None\n",
    "       self.X_subset=None\n",
    "\n",
    "    def fit_PCA(self, X):\n",
    "        # assigns the vectors\n",
    "        _=self.choose_subset(X)\n",
    "\n",
    "        K=self.kernel(self.X_subset,self.X_subset)\n",
    "\n",
    "        #We have to center the kernel\n",
    "        #center = (I-U)K(I-U)\n",
    "        \n",
    "        U=np.ones(K.shape)\n",
    "        I=np.eye(K.shape[0])\n",
    "        K=(I-U)@K@(I-U)\n",
    "        \n",
    "        valp,vectp=np.linalg.eigh(K)\n",
    "\n",
    "        #we take the last r eingenvalues\n",
    "        lmbda=valp[-self.p:]\n",
    "        inverse=1/np.sqrt(lmbda)\n",
    "\n",
    "        self.alphas=(inverse*vectp[:,-self.p:]).T\n",
    "    \n",
    "\n",
    "    def approximated_repr(self,x):\n",
    "        return self.alphas@self.kernel(self.X_subset,x)\n",
    "\n",
    "    def appro_kernel(self,X,Y):\n",
    "        repr_X=self.alphas@self.kernel(self.X_subset,X)\n",
    "        repr_Y=self.alphas@self.kernel(self.X_subset,Y)\n",
    "        return repr_X.T@repr_Y\n",
    "\n",
    "    def choose_subset(self,X):\n",
    "        self.n=X.shape[0]\n",
    "        set=np.random.choice(self.n,self.m)\n",
    "        self.X_subset=X[set]\n",
    "        return set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_onevsall(Y,k):\n",
    "    \"\"\"  \n",
    "    Transform the labels of class k in 1 and the labels of the others classes in -1 \n",
    "    \"\"\"\n",
    "    Y_onevall=-np.ones_like(Y)\n",
    "    Y_onevall[Y==k]=1\n",
    "    return Y_onevall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel computed\n",
      "begin opti :\n"
     ]
    }
   ],
   "source": [
    "kernel=RBF().kernel\n",
    "classifier=KernelSVC(C=1,kernel=kernel)\n",
    "classifier.fit(Xtr,Ytr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
